{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis pipeline template "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plan: \n",
    "1. data cleaning\n",
    "2. EDA\n",
    "3. feature engineering and selection\n",
    "4. deploy several ML models\n",
    "5. hyperparameter tuning on best model \n",
    "6. evaluate best model on test set\n",
    "7. interpret model results\n",
    "8. draw conclusions and document work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're missing any of the below libraries, install here. \n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost #fill w/ library name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and load/visualize data via pd dataframe\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import shap\n",
    "import sklearn.metrics as evaluation\n",
    "\n",
    "#pd.read_csv('blah.csv')\n",
    "#pd.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe dataframe\n",
    "df.describe\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. data cleaning (if necessary) and feature engineering \n",
    "## one hot encoding \n",
    "## imputation of missing values (drop features or fill w/ mean?)\n",
    "\n",
    "#pd.get_dummies(pd.Series(x))\n",
    "#pd.fillna(df[])\n",
    "#pd.isna(df[])\n",
    "#pd.notna(df[])\n",
    "\n",
    "test = pd.DataFrame\n",
    "array = np.array([1, 3, 5, 6])\n",
    "y_array = np.array([3, 5 ,6, 2])\n",
    "plt.scatter(array,y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. EDA \n",
    "## feature distributions (histograms)\n",
    "## correlation matrix (look at collinearity between features and predictors)\n",
    "## check if predicted class is normally distributed \n",
    "\n",
    "plt.hist()\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. feature engineering \n",
    "# build design matrix\n",
    "# separate into test,train sets \n",
    "X_train,X_test,y_train,y_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. deploying several ML models\n",
    "## categorical or numerical prediction?\n",
    "## supervised vs unsupervised?\n",
    "## linear vs nonlinear?\n",
    "\n",
    "xgb_model = XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.001, random_state=0)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. hyperparameter tuning on best models \n",
    "# do this via:\n",
    "# a) from sklearn.model_selection import cross_val_score\n",
    "# b) from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#hyper parameter tuning.Selecting best K\n",
    "neighbors = [x for x in range(1,50) if x % 2 != 0]\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. evaluate best model on test set\n",
    "## ROC curves / F1 statistics / Matthew's correlation coefficient \n",
    "evaluation.f1(y_test,xgb_model.predict(y_test))\n",
    "evaluation.roc_auc\n",
    "evaluation.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. interpret model results \n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, features=X_train, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. conclusions and final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
