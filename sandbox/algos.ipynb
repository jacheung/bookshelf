{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scenarios where finding the shortest path is not a priority and memory is limited, DFS may be the better option. \n",
    "\n",
    "Conversely, when the shortest path is crucial and memory is not a significant constraint, BFS is generally preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DFS leans on STACK DATA STRUCTURES\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name, children=None):\n",
    "        self.name = name\n",
    "        self.children = children or []\n",
    "\n",
    "def dfs(node, target):\n",
    "    if node.name == target:\n",
    "        return node\n",
    "    for child in node.children:\n",
    "        found = dfs(child, target)\n",
    "        if found:\n",
    "            return found\n",
    "    return None\n",
    "\n",
    "# Sample tree structure\n",
    "root = Node(\"Grandpa\", [\n",
    "    Node(\"Dad\", [Node(\"Alice\"), Node(\"Bob\")]),\n",
    "    Node(\"Aunt\", [Node(\"Charlie\")])\n",
    "])\n",
    "\n",
    "found = dfs(root, \"Dan\")\n",
    "if found:\n",
    "    print(f\"Found {found.name}!\")\n",
    "else:\n",
    "    print(\"Alice not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest path: A -> C -> F\n"
     ]
    }
   ],
   "source": [
    "# BFS leans on QUEUE DATA STRUCTURES\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs(graph, start, target):\n",
    "    queue = deque([(start, [start])])\n",
    "    visited = set()\n",
    "    while queue:\n",
    "        node, path = queue.popleft()\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            if node == target:\n",
    "                return path\n",
    "            for neighbor in graph[node]:\n",
    "                queue.append((neighbor, path + [neighbor]))\n",
    "    return None\n",
    "\n",
    "# Sample graph (social network)\n",
    "graph = {\n",
    "    \"A\": [\"B\", \"C\"],\n",
    "    \"B\": [\"A\", \"D\", \"E\"],\n",
    "    \"C\": [\"A\", \"F\"],\n",
    "    \"D\": [\"B\"],\n",
    "    \"E\": [\"B\", \"F\"],\n",
    "    \"F\": [\"C\", \"E\"]\n",
    "}\n",
    "\n",
    "path = bfs(graph, \"A\", \"F\")\n",
    "if path:\n",
    "    print(f\"Shortest path: {' -> '.join(path)}\")\n",
    "else:\n",
    "    print(\"No path found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target found at index: 5\n"
     ]
    }
   ],
   "source": [
    "def binary_search(array, target):\n",
    "    low = 0\n",
    "    high = len(array) - 1\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if array[mid] == target:\n",
    "            return mid\n",
    "        elif array[mid] < target:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "    return -1\n",
    "\n",
    "# Example usage:\n",
    "array = [2, 5, 8, 12, 16, 23, 38, 56, 72, 91]\n",
    "target = 23\n",
    "index = binary_search(array, target)\n",
    "\n",
    "if index != -1:\n",
    "    print(f\"Target found at index: {index}\")\n",
    "else:\n",
    "    print(\"Target not found in the array.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "\n",
    "# Replace with the appropriate model names\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\")\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\")\n",
    "\n",
    "# Function to answer a question based on the PDF content\n",
    "def answer_question(question, pdf_text):\n",
    "    inputs = tokenizer(question, text=pdf_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        doc_scores, retrieved_docs = retriever(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            retrieved_doc_embeds=retrieved_docs,\n",
    "            doc_scores=doc_scores,\n",
    "        )\n",
    "    answer = tokenizer.decode(outputs.sequences, skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "question = \"What is the main idea of Chapter 3?\"\n",
    "pdf_text = # Load the text content from your PDF here\n",
    "answer = answer_question(question, pdf_text)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaled-ML-deployment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
