{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84fc31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcheung/.virtualenvs/thick-ML-deployment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db4af5",
   "metadata": {},
   "source": [
    "## MLFlow\n",
    "MLFLow has 4 different applications: Tracking, Registry, Models, Projects. In this script, we'll demo all 4 via a local environment. \n",
    "1. Tracking: we will log hyper-parameters used and metrics for a transfer-learning model tuned to MNIST.  \n",
    "2. Registry: after our tuned model is identified, we will register our model. \n",
    "3. Models: a stored model must be served to deliver value. Models will be leveraged for inference. \n",
    "4. Projects: DS code is packaged to reproduce runs on any platform/machine via Projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6586651e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///Users/jcheung/Documents/GitHub/bookshelf/deployment_demos/mlruns'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()  \n",
    "# mlflow.set_tracking_uri('http://mlflow-server.kubeflow.svc.cluster.local:5000')  \n",
    "# mlflow.set_registry_uri('http://minio.kubeflow.svc.cluster.local:9000')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a36b2",
   "metadata": {},
   "source": [
    "### First we'll build all the functions for our ML pipeline.\n",
    "This'll include functions for data loading, preprocessing, and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9162be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_tfds():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999038d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mnist_tfds(image, label=None):\n",
    "    # reshape and upsample to 3 channel for transfer learning models\n",
    "    # ... for when no channel information is present\n",
    "    if len(image.shape) != 3:\n",
    "        image = np.dstack((image, image, image))\n",
    "    # ... for when channel is only 1 dimension\n",
    "    if image.shape[2] == 1:\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "    # normalize pixel values\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    # resize with pad for mobilenetv2\n",
    "    image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2aef92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(mlflow.pyfunc.PythonModel):     \n",
    "    def fit(self, xy_tuple_train, xy_tuple_test, hyperparameters):\n",
    "        ## Build model\n",
    "        # class names for mnist hardcoded\n",
    "        class_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "        # set layer regularization for DNN\n",
    "        regularizer = tf.keras.regularizers.l1_l2(hyperparameters['l1'], hyperparameters['l2'])\n",
    "\n",
    "        # load in mobilenetv2 weights and instantiate dense classification head \n",
    "        base_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "        layers = [\n",
    "            hub.KerasLayer(\n",
    "                base_model,\n",
    "                input_shape=(224, 224, 3),\n",
    "                trainable=False,\n",
    "                name='mobilenet_embedding'),\n",
    "            tf.keras.layers.Dense(hyperparameters['num_hidden'],\n",
    "                                  kernel_regularizer=regularizer,\n",
    "                                  activation='relu',\n",
    "                                  name='dense_hidden'),\n",
    "            tf.keras.layers.Dense(len(class_names),\n",
    "                                  kernel_regularizer=regularizer,\n",
    "                                  activation='softmax',\n",
    "                                  name='mnist_prob')\n",
    "        ]\n",
    "\n",
    "        self._model = tf.keras.Sequential(layers, name='mnist-classification')\n",
    "\n",
    "        # compile model \n",
    "        self._model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparams['learning_rate']),\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                            from_logits=False),\n",
    "                            metrics=['accuracy'])\n",
    "                      \n",
    "        ## Fit model\n",
    "        # fit model and save history to model store\n",
    "        self._train_history = self._model.fit(xy_tuple_train, epochs=hyperparameters['epochs'], validation_data=xy_tuple_test)\n",
    "        self._model_base = base_model\n",
    "        \n",
    "    def predict(self, context, model_input: np.ndarray) -> np.ndarray:\n",
    "        image, _ = preprocess_mnist_tfds(model_input)\n",
    "        image = tf.reshape(image, [1, 224, 224, 3])\n",
    "        return self._model.predict(image).argmax()\n",
    "\n",
    "\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d567d8d",
   "metadata": {},
   "source": [
    "### Tracking your model\n",
    "#### MLFlow has two levels for organizing projects:\n",
    "1. At the top level we have \"experiments\". These should be named as \"project-task-version\" (e.g. mnist-classification)\n",
    "2. At the lower level we have \"runs\". A run consists of logging hyperparameters and metrics when models are trained. Multiple runs can be stored within an experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50b9ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment already exists, setting experiment to mnist-classification\n",
      "---------------------\n",
      "Experiment details are:\n",
      "Name: mnist-classification\n",
      "Experiment_id: 661060572347012195\n",
      "Artifact Location: file:///Users/jcheung/Documents/GitHub/bookshelf/deployment_demos/mlruns/661060572347012195\n",
      "Tags: {'version': 'v0.1'}\n",
      "Lifecycle_stage: active\n",
      "Creation timestamp: 1684258817271\n"
     ]
    }
   ],
   "source": [
    "# mlflow Tracking requires definition of experiment name AND logged params\n",
    "# Experiment names they should be defined as \"project-task-version\"\n",
    "experiment_name = \"mnist-classification\"\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        experiment_name,\n",
    "        tags={\"version\": \"v0.1\"},\n",
    "    )\n",
    "except mlflow.exceptions.MlflowException as e: \n",
    "    if str(e) == f\"Experiment '{experiment_name}' already exists.\":\n",
    "        print(f'Experiment already exists, setting experiment to {experiment_name}')\n",
    "        experiment_info = mlflow.set_experiment(experiment_name)\n",
    "        experiment_id = experiment_info.experiment_id\n",
    "\n",
    "experiment = mlflow.get_experiment(experiment_id)\n",
    "print(\"---------------------\")\n",
    "print('Experiment details are:')\n",
    "print(\"Name: {}\".format(experiment.name))\n",
    "print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "print(\"Tags: {}\".format(experiment.tags))\n",
    "print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
    "print(\"Creation timestamp: {}\".format(experiment.creation_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c10969a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and define batch sizes for tensorflow \n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "model = MNIST()\n",
    "ds_train = ds_train.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_test = ds_test.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce4912d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74397177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/16 13:58:49 INFO mlflow.types.utils: Unsupported type hint: <class 'numpy.ndarray'>, skipping schema inference\n",
      "2023/05/16 13:58:49 INFO mlflow.types.utils: Unsupported type hint: <class 'numpy.ndarray'>, skipping schema inference\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c175ea1e-cf62-4a4f-ac47-906c7be8bcc1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c175ea1e-cf62-4a4f-ac47-906c7be8bcc1/assets\n"
     ]
    }
   ],
   "source": [
    "# log a base model \n",
    "hyperparams = {\n",
    "    'learning_rate': 0.01,\n",
    "    'l1': 0.0,\n",
    "    'l2': 0.0, \n",
    "    'num_hidden': 16,\n",
    "    'epochs': 10}\n",
    "\n",
    "# Good practice to explicitly define experiment_id and run_name. \n",
    "# Experiment_id can be extracted from above. \n",
    "# Run name examples (e.g. Linear Regression Default Hyperparams).\n",
    "mlflow_run_name='MNIST Xfer Learning Base'\n",
    "with mlflow.start_run(experiment_id=experiment_id, \n",
    "                      run_name=mlflow_run_name) as run:\n",
    "    # You can set autolog for tensorflow model.\n",
    "    # Note that autolog does not allow logging of any additional params and metrics.\n",
    "    # We'll choose to do manual logging.\n",
    "    # mlflow.tensorflow.autolog()\n",
    "\n",
    "#     model.fit(ds_train, ds_test, hyperparams)\n",
    "\n",
    "    # MLFlow Tracking parameters\n",
    "    mlflow.log_params(params=hyperparams)\n",
    "    \n",
    "    # MLFlow Tracking metrics \n",
    "    # Logging metrics for each epoch (housed in dictionary)\n",
    "    training_history = model._train_history.history\n",
    "    for epoch in range(0, hyperparams['epochs']):\n",
    "        insert = {}\n",
    "        for metric, value in training_history.items():\n",
    "            insert[metric] = training_history[metric][epoch]\n",
    "        mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "    # MLFlow tracking artifact (e.g. model file)\n",
    "    # this will log the model and all its details under run_id/artifacts\n",
    "    mlflow.pyfunc.log_model(python_model=model,\n",
    "                           artifact_path=\"\")\n",
    "\n",
    "    # Close out MLFlow run to prevent any log contamination.\n",
    "    mlflow.end_run(status='FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53821fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "# hyperparameters search using Optuna\n",
    "# can scale Optuna with Kubeflow https://medium.com/optuna/parallel-hyperparameter-tuning-with-optuna-and-kubeflow-pipelines-4ef05ce614ae\n",
    "def objective(trial): \n",
    "    \"\"\"\n",
    "    Optuna objective function for tuning transfer learning model\n",
    "    \"\"\"\n",
    "    hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.1, log=True),\n",
    "        'l1': trial.suggest_float('l1', 0.0, 0.1),\n",
    "        'l2': trial.suggest_float('l2', 0.0, 0.1),\n",
    "        'num_hidden': trial.suggest_int('num_hidden', 8, 64),\n",
    "        'epochs': trial.suggest_int('epochs', 2, 5)\n",
    "    }\n",
    "\n",
    "    model.train(ds_train, ds_test, hyperparams)\n",
    "    training_history = model._train_history.history\n",
    "    validation_accuracy = training_history['val_accuracy'][-1]\n",
    "    return validation_accuracy\n",
    "\n",
    "study = optuna.create_study(study_name='mnist-classification',\n",
    "                            pruner=optuna.pruners.HyperbandPruner(),\n",
    "                            direction='maximize')\n",
    "study.optimize(objective, n_trials=6, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f915bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the hyper parameter tuned model \n",
    "# No need to set experiment ID if you've set above. \n",
    "# Set a run name here(e.g. Linear Regression Default Hyperparams).\n",
    "mlflow_run_name='MNIST Xfer Learning Hyperparam tuned'\n",
    "with mlflow.start_run(experiment_id=experiment_id, \n",
    "                      run_name=mlflow_run_name) as run:\n",
    "    \n",
    "    model.train(ds_train, ds_test, hyperparams)\n",
    "\n",
    "    # MLFlow Tracking parameters\n",
    "    mlflow.log_params(params=hyperparams)\n",
    "    \n",
    "    # MLFlow Tracking metrics \n",
    "    # Logging metrics for each epoch (housed in dictionary)\n",
    "    training_history = model._train_history.history\n",
    "    for epoch in range(0, hyperparams['epochs']):\n",
    "        insert = {}\n",
    "        for metric, value in training_history.items():\n",
    "            insert[metric] = training_history[metric][epoch]\n",
    "        mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "    # MLFlow tracking artifact (e.g. model file)\n",
    "    # mlflow.log_artifact(self._model)\n",
    "\n",
    "    # we need to define how we use tags for testing or if we even need them...\n",
    "    mlflow.set_tag(key=\"test\",\n",
    "                   value=\"manual-logging\")\n",
    "\n",
    "    mlflow.end_run(status='FINISHED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe297d",
   "metadata": {},
   "source": [
    "### Registering your model\n",
    "Once you've run a couple experiments (e.g. hyper parameter tuning) we can select the best model and register that. Model registering just means that we decide this our optimized model to save. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "507ec055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search runs \n",
    "filtering_clause = 'params.epochs = \"10\" and params.learning_rate = \"0.01\"'\n",
    "run = mlflow.search_runs(\n",
    "    experiment_names=['mnist-classification'],\n",
    "    filter_string=filtering_clause,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.val_accuracy DESC\"],\n",
    ")\n",
    "\n",
    "# best performing model run_id. This'll be used to register our model\n",
    "best_run_id = run['run_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6987f1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6970ace0f7764536a6125e05008c130e'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4c2a8f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'mnist-classification' already exists. Creating a new version of this model...\n",
      "2023/05/16 14:00:04 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: mnist-classification, version 2\n",
      "Created version '2' of model 'mnist-classification'.\n"
     ]
    }
   ],
   "source": [
    "# register model after a scanning a couple different runs from your experiment \n",
    "model_name = f'{experiment.name}'\n",
    "# mlflow.tensorflow.log_model\n",
    "mv = mlflow.register_model(model_uri=f\"runs:/{best_run_id}/\",\n",
    "                           name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c3143",
   "metadata": {},
   "source": [
    "### Serving model\n",
    "After we've registered our model, we can now test inference for that model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06f49af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///Users/jcheung/Documents/GitHub/bookshelf/deployment_demos/mlruns/3a2c4fb424004539b8e79f4074145851/artifacts/mnist-classification'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{mlflow.get_tracking_uri()}/{best_run_id}/artifacts/mnist-classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b9493c05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load directly from registry \n",
    "# model_version=1\n",
    "# model_name = f'{experiment.name}'\n",
    "# model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "# load using artifact path directory\n",
    "results = mlflow.search_registered_models(filter_string='name = \"mnist-classification\"')\n",
    "latest_model_details = results[0].latest_versions[0]\n",
    "model = mlflow.pyfunc.load_model(model_uri=f'{latest_model_details.source[7:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "10ed6ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "True:6 and predicted:6\n"
     ]
    }
   ],
   "source": [
    "# randomly sample and load a MNIST JPEG\n",
    "import os \n",
    "from random import sample\n",
    "from PIL import Image\n",
    "fp = '/Users/jcheung/Documents/GitHub/thin-ML-deployment/app/ml/test_images'\n",
    "files = [f'{fp}/{x}' for x in os.listdir(fp) if x.split('.')[-1] == 'jpg']\n",
    "filename = sample(files, 1)[0]\n",
    "image = np.array(Image.open(filename))\n",
    "\n",
    "# predict using custom mlflow model\n",
    "predicted = model.predict(image)\n",
    "\n",
    "# output and compare results\n",
    "true = filename.split('/')[-1].split('_')[-1][0]\n",
    "\n",
    "print(f'True:{true} and predicted:{predicted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5f07de0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thick-ML-deployment/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py:427\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _MLFLOW_OPENAI_TESTING\u001b[38;5;241m.\u001b[39mget():\n\u001b[1;32m    425\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thick-ML-deployment/lib/python3.9/site-packages/mlflow/pyfunc/model.py:365\u001b[0m, in \u001b[0;36m_PythonModelPyfuncWrapper.predict\u001b[0;34m(self, model_input)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_input):\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[84], line 42\u001b[0m, in \u001b[0;36mMNIST.predict\u001b[0;34m(self, context, model_input)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, context, model_input: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 42\u001b[0m     image, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_mnist_tfds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(image, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mpredict(image)\u001b[38;5;241m.\u001b[39margmax()\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mpreprocess_mnist_tfds\u001b[0;34m(image, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_mnist_tfds\u001b[39m(image, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# reshape and upsample to 3 channel for transfer learning models\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# ... for when no channel information is present\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m      5\u001b[0m         image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdstack((image, image, image))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# ... for when channel is only 1 dimension\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kubeflow.katib as katib\n",
    "\n",
    "# Step 1. Create an objective function.\n",
    "def objective(hyperparams):\n",
    "    training_history = model.train(ds_train, ds_test, hyperparams)\n",
    "    validation_accuracy = training_history.history['val_accuracy'][-1]\n",
    "    # Katib parses metrics in this format: <metric-name>=<metric-value>.\n",
    "    print(f\"result={result}\")\n",
    "\n",
    "# Step 2. Create HyperParameter search space.\n",
    "hyperparams = {\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.1, log=True),\n",
    "    'l1': trial.suggest_float('l1', 0.0, 1),\n",
    "    'l2': trial.suggest_float('l2', 0.0, 1),\n",
    "    'num_hidden': katib.search.int(min=8, max=64),\n",
    "    'epochs': katib.search.int(min=5, max=10)\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    \"a\": katib.search.int(min=10, max=20),\n",
    "    \"b\": katib.search.double(min=0.1, max=0.2)\n",
    "}\n",
    "\n",
    "# Step 3. Create Katib Experiment.\n",
    "katib_client = katib.KatibClient()\n",
    "name = \"tune-experiment\"\n",
    "katib_client.tune(\n",
    "    name=name,\n",
    "    objective=objective,\n",
    "    parameters=parameters,\n",
    "    objective_metric_name=\"result\",\n",
    "    max_trial_count=12\n",
    ")\n",
    "\n",
    "# Step 4. Get the best HyperParameters.\n",
    "print(katib_client.get_optimal_hyperparameters(name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thick-ML-deployment",
   "language": "python",
   "name": "thick-ml-deployment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
